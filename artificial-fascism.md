# Artificial Fascism
`2021-08-15`

The power of automated judgement. [What could possibly go wrong?](https://www.youtube.com/watch?v=TstteJ1eIZg)

## Face detection

### The Best Algorithms Struggle to Recognize Black Faces Equally
> US government tests find even top-performing facial recognition systems misidentify blacks at rates five to 10 times higher than they do whites.
>
> &mdash; [Wired, 2019-07-22](https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/)

### Twitter
> Trying a horrible experiment...
>
> Which will the Twitter algorithm pick: Mitch McConnell or Barack Obama?
>
> &mdash; [Tony Arcieri (@bascule), 2020-09-19](https://twitter.com/bascule/status/1307440596668182528)"

### Zoom
> Turns out @zoom_us has a crappy face-detection algorithm that erases black faces...and determines that a nice pale globe in the background must be a better face than what should be obvious.
>
> &mdash; [Colin Madland (@colinmadland), 2020-09-19](https://twitter.com/colinmadland/status/1307111825851416577)

## Credit score

### Bias isn’t the only problem with credit scores - and no, AI can’t help
> The biggest ever study of real people's mortgage data shows that predictive tools are not simply biased for minority and low income groups, but less accurate too.
>
> &mdash; [Technology Review, 2021-06-17](https://www.technologyreview.com/2021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness-machine-learning/)

## Fraud detection

### Machine Learning for Credit Card Fraud detection
> ML for credit card fraud detection is one of those fields where most of the published research is unfortunately not reproducible. Real-world transaction data cannot be shared for confidentiality reasons, but we also believe authors do not make enough efforts to provide their code and make their results reproducible.
>
> &mdash; [Towards Data Science, 2021-05-26](https://towardsdatascience.com/machine-learning-for-credit-card-fraud-detection-a-jupyter-book-for-reproducible-research-8ca5edad7b5d)

## Performance assessment

### Xsolla lays off 150 after an algorithm ruled staff 'unengaged and unproductive'
> Xsolla, a company that provides payment processing options for the game industry, has laid off roughly one-third of its workforce after an algorithm employed by the company decided those 150 individuals were "unengaged and unproductive employees".
>
> &mdash; [Gamasutra, 2021-08-10](https://www.gamasutra.com/view/news/386534/Xsolla_lays_off_150_after_an_algorithm_ruled_staff_unengaged_and_unproductive.php)

### Fired by Bot at Amazon: ‘It’s You Against the Machine’
> Stephen Normandin spent almost four years racing around Phoenix delivering packages as a contract driver for Amazon.com Inc. Then one day, he received an automated email. The algorithms tracking him had decided he wasn’t doing his job properly.
>
> &mdash; [Bloomberg, 2021-06-28](https://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine-managers-and-workers-are-losing-out)

## Sentiment analysis

### Indigo Airline’s Twitter fiasco — Sentiment classification gone wrong
> Indigo airline’s (leading private airline in India) Twitter reply to a customer’s tweet last year gained lot of bad publicity for the airline. A disgruntled customer had tweeted about misplaced baggage, but it was a sarcastic tweet thanking them. Indigo’s reply was to thank the customer.
>
> &mdash; [Practical Data Science And Engineering, 2018-02-20](https://medium.com/practical-data-science-and-engineering/indigo-airlines-twitter-fiasco-sentiment-classification-gone-wrong-5802321468e2)

## Healthcare

### Millions of black people affected by racial bias in health-care algorithms
> An algorithm widely used in US hospitals to allocate health care to patients has been systematically discriminating against black people, a sweeping analysis has found.
>
> &mdash; [Nature, 2019-10-24](https://www.nature.com/articles/d41586-019-03228-6)

### How Algorithms Can Punish the Poor
> As Americans look for greater government efficiencies, we increasingly turn to automated systems that use algorithms to determine who is eligible for access to housing, welfare benefits, intervention from child protective services, and more.
>
> &mdash; [Slate, 2018-03-29](https://slate.com/human-interest/2018/03/automating-inequality-author-virginia-eubanks-on-how-algorithms-can-punish-the-poor.html)

## Law enforcement

### Wrongfully Accused by an Algorithm
> In what may be the first known case of its kind, a faulty facial recognition match led to a Michigan man's arrest for a crime he did not commit.
>
> &mdash; [The New York Times, 2020-08-03](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)

### HU facial recognition software predicts criminality
> A group of Harrisburg University professors and a Ph.D. student have developed automated computer facial recognition software capable of predicting whether someone is likely going to be a criminal.
>
> &mdash; [Harrisburg University, 2020-05-06](https://web.archive.org/web/20200506013352/https://harrisburgu.edu/hu-facial-recognition-software-identifies-potential-criminals/)

### Artificial intelligence is ripe for abuse, tech researcher warns: 'a fascist's dream'
> Crawford was concerned about the potential use of AI in predictive policing systems, which already gather the kind of data necessary to train an AI system. Such systems are flawed, as shown by a Rand Corporation study of Chicago’s program. The predictive policing did not reduce crime, but did increase harassment of people in “hotspot” areas.
>
> &mdash; [The Guardian, 2017-03-13](https://www.theguardian.com/technology/2017/mar/13/artificial-intelligence-ai-abuses-fascism-donald-trump)
